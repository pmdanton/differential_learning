{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Learning in TensorFlow 2\n",
    "This notebook implements the paper [*Differential Machine Learning*](https://arxiv.org/abs/2005.02347) by Brian Huge and Antoine Savine. The authors already provide notebooks [on their github page](https://github.com/differential-machine-learning) but using TensorFlow 1 code, and lots of low-level manual implementation. Here we aim to reproduce the results with high-level APIs (Keras, GradientTape) in TensorFlow 2. We will use the notations from the authors' notebook as much as possible to allow for easy comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Using TensorFlow version\", tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Code Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward neural network in TensorFlow\n",
    "Feedforward networks are easily defined in Keras with the [Sequential API](https://www.tensorflow.org/guide/keras/overview#sequential_model) so we don't need to write our own here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit backpropagation and twin network\n",
    "We will use the [Keras subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class) to define twin networks: given any \"vanilla\" network, its twin version will also return the sensitivities to inputs by AAD. \n",
    "In TensorFlow 2, getting the gradients is easily achieved using [tf.gradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape). \n",
    "We try to keep As we can see, it doesn't take much code and greatly reduces the risk of errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinNetwork(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vanilla_net):\n",
    "        super(TwinNetwork, self).__init__()\n",
    "        self.vanilla_net = vanilla_net\n",
    "\n",
    "    def call(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inputs)\n",
    "            predictions = self.vanilla_net(inputs)\n",
    "        derivs_predictions = tape.gradient(predictions, inputs) \n",
    "        return predictions, derivs_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "By implementing TwinNetwork as a Keras model, we get an important benefit: the [API](https://www.tensorflow.org/guide/keras/train_and_evaluate) implements the training loop for us, and is flexible enough to combine multiple losses on multi-output networks like TwinNetwork. The only thing for us to implement is the weighted mean squared error used as loss function for the sensitivities. We sublass [Keras Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss) to implement this logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMeanSquaredError(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, lambda_j):\n",
    "        super(WeightedMeanSquaredError, self).__init__()\n",
    "        self.lambda_j = tf.reshape(lambda_j, (1,-1))\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.keras.losses.MSE(self.lambda_j*y_true, self.lambda_j*y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "For convenience here we introduce a simple class for data pre-processing, inspired by [scikit's StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) but tailored to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TwinScaler():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x_scaler = StandardScaler()\n",
    "        self.y_scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.x_scaler.fit(x)\n",
    "        self.y_scaler.fit(y)\n",
    "        self.dy_dx_scale = self.x_scaler.scale_ / self.y_scaler.scale_\n",
    "        \n",
    "    def transform(self, x, y, dy_dx):\n",
    "        x_scaled = self.x_scaler.transform(x)\n",
    "        y_scaled = self.y_scaler.transform(y)\n",
    "        dy_dx_scaled = dy_dx * self.dy_dx_scale\n",
    "        return (x_scaled, y_scaled, dy_dx_scaled)\n",
    "        \n",
    "    def inverse_transform(self, y_scaled, dy_dx_scaled):\n",
    "        y = self.y_scaler.inverse_transform(y_scaled)\n",
    "        dy_dx = dy_dx_scaled / self.dy_dx_scale\n",
    "        return (y, dy_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement helper functions to calculate the weights *lambda_j* in the loss on sensitivities, and the coefficients *alpha* and *beta* to combine the loss on prices and the loss on sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lambda_j(dy_dx_scaled):\n",
    "    return 1.0/tf.math.sqrt(tf.reduce_mean(tf.square(dy_dx_scaled), axis=0))\n",
    "\n",
    "def calc_alpha_beta(n, lambda_hyperparameter= 1):\n",
    "    alpha = 1/(1+lambda_hyperparameter*n)\n",
    "    beta = 1-alpha\n",
    "    return [alpha, beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Learning Pricing and Risk Functions from LSM samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black & Scholes\n",
    "The following code is taken from the [original notebook](https://github.com/differential-machine-learning) to generate the Black-Scholes call option example in 1 dimension. We don't modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper analytics    \n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bsPrice(spot, strike, vol, T):\n",
    "    d1 = (np.log(spot/strike) + vol * vol * T) / vol / np.sqrt(T)\n",
    "    d2 = d1 - vol * np.sqrt(T)\n",
    "    return spot * norm.cdf(d1) - strike * norm.cdf(d2)\n",
    "\n",
    "def bsDelta(spot, strike, vol, T):\n",
    "    d1 = (np.log(spot/strike) + vol * vol * T) / vol / np.sqrt(T)\n",
    "    return norm.cdf(d1)\n",
    "\n",
    "def bsVega(spot, strike, vol, T):\n",
    "    d1 = (np.log(spot/strike) + vol * vol * T) / vol / np.sqrt(T)\n",
    "    return spot * np.sqrt(T) * norm.pdf(d1)\n",
    "#\n",
    "    \n",
    "# main class\n",
    "class BlackScholes:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vol=0.2,\n",
    "                 T1=1, \n",
    "                 T2=2, \n",
    "                 K=1.10,\n",
    "                 volMult=1.5):\n",
    "        \n",
    "        self.spot = 1\n",
    "        self.vol = vol\n",
    "        self.T1 = T1\n",
    "        self.T2 = T2\n",
    "        self.K = K\n",
    "        self.volMult = volMult\n",
    "                        \n",
    "    # training set: returns S1 (mx1), C2 (mx1) and dC2/dS1 (mx1)\n",
    "    def trainingSet(self, m, anti=True, seed=None):\n",
    "    \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # 2 sets of normal returns\n",
    "        returns = np.random.normal(size=[m, 2])\n",
    "\n",
    "        # SDE\n",
    "        vol0 = self.vol * self.volMult\n",
    "        R1 = np.exp(-0.5*vol0*vol0*self.T1 + vol0*np.sqrt(self.T1)*returns[:,0])\n",
    "        R2 = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) \\\n",
    "                    + self.vol*np.sqrt(self.T2-self.T1)*returns[:,1])\n",
    "        S1 = self.spot * R1\n",
    "        S2 = S1 * R2 \n",
    "\n",
    "        # payoff\n",
    "        pay = np.maximum(0, S2 - self.K)\n",
    "        \n",
    "        # two antithetic paths\n",
    "        if anti:\n",
    "            \n",
    "            R2a = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) \\\n",
    "                    - self.vol*np.sqrt(self.T2-self.T1)*returns[:,1])\n",
    "            S2a = S1 * R2a             \n",
    "            paya = np.maximum(0, S2a - self.K)\n",
    "            \n",
    "            X = S1\n",
    "            Y = 0.5 * (pay + paya)\n",
    "    \n",
    "            # differentials\n",
    "            Z1 =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) \n",
    "            Z2 =  np.where(S2a > self.K, R2a, 0.0).reshape((-1,1)) \n",
    "            Z = 0.5 * (Z1 + Z2)\n",
    "                    \n",
    "        # standard\n",
    "        else:\n",
    "        \n",
    "            X = S1\n",
    "            Y = pay\n",
    "            \n",
    "            # differentials\n",
    "            Z =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) \n",
    "        \n",
    "        return X.reshape([-1,1]), Y.reshape([-1,1]), Z.reshape([-1,1])\n",
    "    \n",
    "    # test set: returns a grid of uniform spots \n",
    "    # with corresponding ground true prices, deltas and vegas\n",
    "    def testSet(self, lower=0.35, upper=1.65, num=100, seed=None):\n",
    "        \n",
    "        spots = np.linspace(lower, upper, num).reshape((-1, 1))\n",
    "        # compute prices, deltas and vegas\n",
    "        prices = bsPrice(spots, self.K, self.vol, self.T2 - self.T1).reshape((-1, 1))\n",
    "        deltas = bsDelta(spots, self.K, self.vol, self.T2 - self.T1).reshape((-1, 1))\n",
    "        vegas = bsVega(spots, self.K, self.vol, self.T2 - self.T1).reshape((-1, 1))\n",
    "        return spots, prices, deltas, vegas   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us our training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BlackScholes()\n",
    "bs_sims = bs.trainingSet(8192, anti=True, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_scaler = TwinScaler()\n",
    "bs_scaler.fit(bs_sims[0], bs_sims[1])\n",
    "bs_scaled = bs_scaler.transform(*bs_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the BlackScholes class is built with NumPy that uses float64 by default, while TensorFlow uses float32. Matrix multiplication will throw if the two types are mixed up, so we explicitely cast to tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_scaled = list(map(lambda x: tf.cast(x, tf.float32), bs_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate lambda_j, create the corresponding weighted MSE loss object, and calculate the relative weights of price loss and sensitivities loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_j = calc_lambda_j(bs_scaled[-1])\n",
    "weighted_mse = WeightedMeanSquaredError(lambda_j)\n",
    "\n",
    "[alpha, beta] = calc_alpha_beta(bs_scaled[0].shape[1], lambda_hyperparameter= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a simple neural net, here with 4 hidden layers of size 20 and softplus activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_net = Sequential(\n",
    "    [Dense(20, activation='softplus', input_shape=(1,)), \n",
    "     Dense(20, activation='softplus'), \n",
    "     Dense(20, activation='softplus'), \n",
    "     Dense(20, activation='softplus'), \n",
    "     Dense(1, activation=None)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert it to a twin network with a simple call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twin_net = TwinNetwork(vanilla_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compilation step lets use choose the optimizer and, more importantly, the loss for each head of the model (prices, sensitivities), as well as loss_weights the combine the losses. In our case they are alpha and beta calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twin_net.compile(optimizer=\"adam\", loss=[\"mse\", weighted_mse], loss_weights=[alpha, beta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "STEPS_PER_EPOCH = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the learning rate scheduler using a [Keras callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule_epochs = [0.0, 0.2, 0.6, 0.9, 1.0]\n",
    "lr_schedule_rates = [1e-8, 0.1, 0.01, 1e-6, 1e-8]\n",
    "lr_schedule_fn = lambda t: np.interp(t/(EPOCHS-1), lr_schedule_epochs, lr_schedule_rates)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our model. To train on multiple outputs we simply pass them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.9736 - output_1_loss: 0.9822 - output_2_loss: 0.9650 - lr: 1.0000e-08\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6738 - output_1_loss: 0.6585 - output_2_loss: 0.6891 - lr: 0.0051\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2641 - output_1_loss: 0.2049 - output_2_loss: 0.3233 - lr: 0.0101\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1594 - output_1_loss: 0.1143 - output_2_loss: 0.2044 - lr: 0.0152\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1370 - output_1_loss: 0.1063 - output_2_loss: 0.1677 - lr: 0.0202\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1283 - output_1_loss: 0.1067 - output_2_loss: 0.1500 - lr: 0.0253\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1231 - output_1_loss: 0.1017 - output_2_loss: 0.1445 - lr: 0.0303\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1304 - output_1_loss: 0.1046 - output_2_loss: 0.1561 - lr: 0.0354\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 966us/step - loss: 0.1347 - output_1_loss: 0.1140 - output_2_loss: 0.1553 - lr: 0.0404\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1203 - output_1_loss: 0.0958 - output_2_loss: 0.1448 - lr: 0.0455\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1264 - output_1_loss: 0.1032 - output_2_loss: 0.1496 - lr: 0.0505\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1274 - output_1_loss: 0.1040 - output_2_loss: 0.1507 - lr: 0.0556\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 971us/step - loss: 0.1325 - output_1_loss: 0.1058 - output_2_loss: 0.1591 - lr: 0.0606\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1334 - output_1_loss: 0.1158 - output_2_loss: 0.1510 - lr: 0.0657\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.1377 - output_1_loss: 0.1168 - output_2_loss: 0.1587 - lr: 0.0707\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1254 - output_1_loss: 0.1044 - output_2_loss: 0.1464 - lr: 0.0758\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1187 - output_1_loss: 0.0940 - output_2_loss: 0.1435 - lr: 0.0808\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1309 - output_1_loss: 0.1078 - output_2_loss: 0.1541 - lr: 0.0859\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1271 - output_1_loss: 0.1064 - output_2_loss: 0.1478 - lr: 0.0909\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1386 - output_1_loss: 0.1157 - output_2_loss: 0.1615 - lr: 0.0960\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1258 - output_1_loss: 0.1066 - output_2_loss: 0.1450 - lr: 0.0995\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1298 - output_1_loss: 0.1032 - output_2_loss: 0.1565 - lr: 0.0973\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1233 - output_1_loss: 0.0972 - output_2_loss: 0.1494 - lr: 0.0950\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1299 - output_1_loss: 0.1061 - output_2_loss: 0.1536 - lr: 0.0927\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1368 - output_1_loss: 0.1176 - output_2_loss: 0.1560 - lr: 0.0905\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1261 - output_1_loss: 0.1018 - output_2_loss: 0.1504 - lr: 0.0882\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1258 - output_1_loss: 0.1049 - output_2_loss: 0.1468 - lr: 0.0859\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1248 - output_1_loss: 0.0996 - output_2_loss: 0.1500 - lr: 0.0836\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1272 - output_1_loss: 0.1019 - output_2_loss: 0.1525 - lr: 0.0814\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1234 - output_1_loss: 0.1018 - output_2_loss: 0.1450 - lr: 0.0791\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1201 - output_1_loss: 0.0990 - output_2_loss: 0.1412 - lr: 0.0768\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1272 - output_1_loss: 0.1000 - output_2_loss: 0.1544 - lr: 0.0745\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1237 - output_1_loss: 0.1002 - output_2_loss: 0.1472 - lr: 0.0723\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1211 - output_1_loss: 0.0959 - output_2_loss: 0.1464 - lr: 0.0700\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1170 - output_1_loss: 0.0883 - output_2_loss: 0.1458 - lr: 0.0677\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1292 - output_1_loss: 0.1081 - output_2_loss: 0.1502 - lr: 0.0655\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1270 - output_1_loss: 0.1025 - output_2_loss: 0.1516 - lr: 0.0632\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1233 - output_1_loss: 0.1009 - output_2_loss: 0.1456 - lr: 0.0609\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1272 - output_1_loss: 0.1011 - output_2_loss: 0.1533 - lr: 0.0586\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1179 - output_1_loss: 0.0954 - output_2_loss: 0.1404 - lr: 0.0564\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1204 - output_1_loss: 0.0968 - output_2_loss: 0.1440 - lr: 0.0541\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1271 - output_1_loss: 0.1027 - output_2_loss: 0.1516 - lr: 0.0518\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1230 - output_1_loss: 0.0980 - output_2_loss: 0.1480 - lr: 0.0495\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1264 - output_1_loss: 0.1045 - output_2_loss: 0.1483 - lr: 0.0473\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1231 - output_1_loss: 0.1011 - output_2_loss: 0.1451 - lr: 0.0450\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1228 - output_1_loss: 0.0964 - output_2_loss: 0.1493 - lr: 0.0427\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1235 - output_1_loss: 0.0994 - output_2_loss: 0.1476 - lr: 0.0405\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1214 - output_1_loss: 0.0968 - output_2_loss: 0.1461 - lr: 0.0382\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1188 - output_1_loss: 0.0926 - output_2_loss: 0.1450 - lr: 0.0359\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1275 - output_1_loss: 0.1050 - output_2_loss: 0.1500 - lr: 0.0336\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1198 - output_1_loss: 0.0953 - output_2_loss: 0.1443 - lr: 0.0314\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1253 - output_1_loss: 0.1008 - output_2_loss: 0.1498 - lr: 0.0291\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1225 - output_1_loss: 0.1015 - output_2_loss: 0.1435 - lr: 0.0268\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1209 - output_1_loss: 0.0931 - output_2_loss: 0.1488 - lr: 0.0245\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1200 - output_1_loss: 0.0955 - output_2_loss: 0.1446 - lr: 0.0223\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1236 - output_1_loss: 0.0991 - output_2_loss: 0.1481 - lr: 0.0200\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1232 - output_1_loss: 0.1014 - output_2_loss: 0.1450 - lr: 0.0177\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1204 - output_1_loss: 0.0933 - output_2_loss: 0.1475 - lr: 0.0155\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1262 - output_1_loss: 0.1057 - output_2_loss: 0.1467 - lr: 0.0132\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1173 - output_1_loss: 0.0886 - output_2_loss: 0.1459 - lr: 0.0109\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1207 - output_1_loss: 0.0961 - output_2_loss: 0.1453 - lr: 0.0098\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1224 - output_1_loss: 0.0979 - output_2_loss: 0.1469 - lr: 0.0095\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1186 - output_1_loss: 0.0973 - output_2_loss: 0.1399 - lr: 0.0091\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1243 - output_1_loss: 0.0966 - output_2_loss: 0.1519 - lr: 0.0088\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1180 - output_1_loss: 0.0944 - output_2_loss: 0.1416 - lr: 0.0085\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1250 - output_1_loss: 0.0996 - output_2_loss: 0.1504 - lr: 0.0081\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1189 - output_1_loss: 0.0917 - output_2_loss: 0.1460 - lr: 0.0078\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1244 - output_1_loss: 0.1028 - output_2_loss: 0.1459 - lr: 0.0074\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1207 - output_1_loss: 0.0969 - output_2_loss: 0.1445 - lr: 0.0071\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1226 - output_1_loss: 0.0977 - output_2_loss: 0.1475 - lr: 0.0068\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1224 - output_1_loss: 0.0945 - output_2_loss: 0.1502 - lr: 0.0064\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1207 - output_1_loss: 0.0996 - output_2_loss: 0.1419 - lr: 0.0061\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1182 - output_1_loss: 0.0927 - output_2_loss: 0.1437 - lr: 0.0058\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 831us/step - loss: 0.1247 - output_1_loss: 0.1012 - output_2_loss: 0.1482 - lr: 0.0054\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 957us/step - loss: 0.1253 - output_1_loss: 0.1028 - output_2_loss: 0.1478 - lr: 0.0051\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 930us/step - loss: 0.1175 - output_1_loss: 0.0911 - output_2_loss: 0.1440 - lr: 0.0047\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1202 - output_1_loss: 0.0940 - output_2_loss: 0.1464 - lr: 0.0044\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1223 - output_1_loss: 0.0993 - output_2_loss: 0.1453 - lr: 0.0041\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1195 - output_1_loss: 0.0951 - output_2_loss: 0.1438 - lr: 0.0037\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1234 - output_1_loss: 0.0988 - output_2_loss: 0.1480 - lr: 0.0034\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1240 - output_1_loss: 0.0992 - output_2_loss: 0.1488 - lr: 0.0031\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1187 - output_1_loss: 0.0945 - output_2_loss: 0.1428 - lr: 0.0027\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1225 - output_1_loss: 0.1009 - output_2_loss: 0.1442 - lr: 0.0024\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1201 - output_1_loss: 0.0927 - output_2_loss: 0.1475 - lr: 0.0021\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 987us/step - loss: 0.1244 - output_1_loss: 0.1004 - output_2_loss: 0.1484 - lr: 0.0017\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 998us/step - loss: 0.1182 - output_1_loss: 0.0934 - output_2_loss: 0.1431 - lr: 0.0014\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1209 - output_1_loss: 0.0953 - output_2_loss: 0.1465 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1216 - output_1_loss: 0.0983 - output_2_loss: 0.1449 - lr: 7.0800e-04\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1191 - output_1_loss: 0.0935 - output_2_loss: 0.1447 - lr: 3.7133e-04\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1233 - output_1_loss: 0.1000 - output_2_loss: 0.1467 - lr: 3.4667e-05\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1271 - output_1_loss: 0.1041 - output_2_loss: 0.1500 - lr: 9.1000e-07\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1154 - output_1_loss: 0.0893 - output_2_loss: 0.1414 - lr: 8.1000e-07\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1218 - output_1_loss: 0.0965 - output_2_loss: 0.1472 - lr: 7.1000e-07\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1206 - output_1_loss: 0.0969 - output_2_loss: 0.1442 - lr: 6.1000e-07\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1219 - output_1_loss: 0.0960 - output_2_loss: 0.1477 - lr: 5.1000e-07\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1205 - output_1_loss: 0.0975 - output_2_loss: 0.1436 - lr: 4.1000e-07\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1180 - output_1_loss: 0.0932 - output_2_loss: 0.1427 - lr: 3.1000e-07\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 920us/step - loss: 0.1244 - output_1_loss: 0.1002 - output_2_loss: 0.1487 - lr: 2.1000e-07\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1177 - output_1_loss: 0.0855 - output_2_loss: 0.1499 - lr: 1.1000e-07\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1247 - output_1_loss: 0.1079 - output_2_loss: 0.1414 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "hist = twin_net.fit(bs_scaled[0], [bs_scaled[1], bs_scaled[2]], \\\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see how our network performs on the testing set. We'll need to scale our data, get the predictions, and de-scale the outputs: we put it all in a helper class *Pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to scale, predict, unscale\n",
    "class Pipeline():\n",
    "    def __init__(self, twin_net, scaler):\n",
    "        self.twin_net = twin_net\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def predict(self, x):\n",
    "        x_scaled = self.scaler.x_scaler.transform(x)\n",
    "        y_scaled = self.twin_net.predict(x_scaled)\n",
    "        y = self.scaler.inverse_transform(*y_scaled)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our prediction pipeline for the twin model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(twin_net, bs_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_test= bs.testSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get our predictions from the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pipe.predict(bs_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot our prediction of prices against closed-form prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prices predictions\n",
    "plt.plot(bs_test[0], test_pred[0], 'r')\n",
    "plt.plot(bs_test[0], bs_test[1], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for delta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delta predictions\n",
    "plt.plot(bs_test[0], test_pred[1], 'r')\n",
    "plt.plot(bs_test[0], bs_test[2], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very good fit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
